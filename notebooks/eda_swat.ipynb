{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T11:44:47.649328Z",
     "start_time": "2025-07-29T11:44:47.638716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# --- Path and Import Setup for your custom functions ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "from preprocess import create_windows_vectorized\n",
    "\n",
    "print(\"All libraries and custom functions imported.\")"
   ],
   "id": "4faca25da4d044a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries and custom functions imported.\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T11:44:48.110761Z",
     "start_time": "2025-07-29T11:44:48.105386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_swat_data(df):\n",
    "    \"\"\"Cleans a raw SWaT DataFrame that has already been loaded.\"\"\"\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['Timestamp'] = df['Timestamp'].str.strip()\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %I:%M:%S %p', errors='coerce')\n",
    "        df.dropna(subset=['Timestamp'], inplace=True)\n",
    "        df = df.set_index('Timestamp')\n",
    "    if 'Normal/Attack' in df.columns:\n",
    "        df['Label'] = (df['Normal/Attack'] != 'Normal').astype(int)\n",
    "        df = df.drop(columns=['Normal/Attack'])\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    return df\n",
    "\n",
    "print(\"Helper function defined.\")"
   ],
   "id": "6521ef95e0f0a30a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function defined.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T11:44:55.874796Z",
     "start_time": "2025-07-29T11:44:48.599329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Define file paths ---\n",
    "normal_data_path = '../data/SWaT/Physical/SWaT_Dataset_Normal_v0.csv'\n",
    "attack_data_path = '../data/SWaT/Physical/SWaT_Dataset_Attack_v0.csv'\n",
    "\n",
    "# --- Load each file, then clean ---\n",
    "normal_df_raw = pd.read_csv(normal_data_path, skiprows=1, low_memory=False)\n",
    "attack_df_raw = pd.read_csv(attack_data_path, low_memory=False)\n",
    "\n",
    "normal_df = clean_swat_data(normal_df_raw)\n",
    "attack_df = clean_swat_data(attack_df_raw)\n",
    "\n",
    "combined_df = pd.concat([normal_df, attack_df])\n",
    "combined_df.sort_index(inplace=True)\n",
    "\n",
    "# --- Feature Selection ---\n",
    "selected_features = [\n",
    "    'FIT101', 'LIT101', 'AIT201', 'AIT202', 'FIT201', 'AIT402',\n",
    "    'FIT401', 'LIT301', 'LIT401', 'AIT502', 'FIT501', 'PIT501'\n",
    "]\n",
    "subset_df = combined_df[selected_features + ['Label']].copy()\n",
    "print(f\"Data loaded and processed. Using {len(selected_features)} features.\")"
   ],
   "id": "d831480175b3ea00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and processed. Using 12 features.\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T11:44:55.998318Z",
     "start_time": "2025-07-29T11:44:55.887280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = subset_df.drop('Label', axis=1)\n",
    "labels = subset_df['Label']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "scaled_df = pd.DataFrame(features_scaled, columns=features.columns, index=features.index)\n",
    "scaled_df['Label'] = labels\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = 50\n",
    "\n",
    "X, y = create_windows_vectorized(scaled_df, window_size=WINDOW_SIZE, stride=STEP_SIZE)\n",
    "print(f\"Windowing complete. Shape of X: {X.shape}\")"
   ],
   "id": "93fa32fec147b439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorized windows created: 18934 sequences of shape (50, 12)\n",
      "Windowing complete. Shape of X: (18934, 50, 12)\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T11:44:56.110231Z",
     "start_time": "2025-07-29T11:44:56.045215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Create the initial splits from the original, imbalanced data\n",
    "# 'stratify=y' ensures a proportional split of attack samples\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Verify the New Distribution\n",
    "print(\"✅ New data split complete:\")\n",
    "print(\"Training set label distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print(\"Validation set label distribution:\", dict(zip(*np.unique(y_val, return_counts=True))))\n",
    "print(\"Test set label distribution:\", dict(zip(*np.unique(y_test, return_counts=True))))"
   ],
   "id": "f274499958435d7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New data split complete:\n",
      "Training set label distribution: {np.int64(0): np.int64(10686), np.int64(1): np.int64(674)}\n",
      "Validation set label distribution: {np.int64(0): np.int64(3562), np.int64(1): np.int64(225)}\n",
      "Test set label distribution: {np.int64(0): np.int64(3562), np.int64(1): np.int64(225)}\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-29T11:44:56.529518Z",
     "start_time": "2025-07-29T11:44:56.177664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_path = '../data/processed'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(save_path, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(save_path, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(save_path, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(save_path, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(save_path, 'y_test.npy'), y_test)\n",
    "\n",
    "# This tells you the correct number to use in your train.py and evaluate.py scripts\n",
    "print(f\"Data saved. The INPUT_DIM for your model is: {X_train.shape[2]}\")"
   ],
   "id": "1eec7913a61d4b1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved. The INPUT_DIM for your model is: 12\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c20a4b539950ec9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
