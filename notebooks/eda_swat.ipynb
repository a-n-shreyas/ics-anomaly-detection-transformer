{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:04:09.831116Z",
     "start_time": "2025-07-21T15:04:09.804948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# --- Path and Import Setup for your custom functions ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "from preprocess import create_windows_vectorized\n",
    "\n",
    "print(\"All libraries and custom functions imported.\")"
   ],
   "id": "4faca25da4d044a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries and custom functions imported.\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:04:10.163181Z",
     "start_time": "2025-07-21T15:04:10.156398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_swat_data(df):\n",
    "    \"\"\"Cleans a raw SWaT DataFrame that has already been loaded.\"\"\"\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['Timestamp'] = df['Timestamp'].str.strip()\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %I:%M:%S %p', errors='coerce')\n",
    "        df.dropna(subset=['Timestamp'], inplace=True)\n",
    "        df = df.set_index('Timestamp')\n",
    "    if 'Normal/Attack' in df.columns:\n",
    "        df['Label'] = (df['Normal/Attack'] != 'Normal').astype(int)\n",
    "        df = df.drop(columns=['Normal/Attack'])\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    return df\n",
    "\n",
    "print(\"Helper function defined.\")"
   ],
   "id": "6521ef95e0f0a30a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function defined.\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:04:17.110841Z",
     "start_time": "2025-07-21T15:04:10.544363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Define file paths ---\n",
    "normal_data_path = '../data/SWaT/Physical/SWaT_Dataset_Normal_v0.csv'\n",
    "attack_data_path = '../data/SWaT/Physical/SWaT_Dataset_Attack_v0.csv'\n",
    "\n",
    "# --- Load each file, then clean ---\n",
    "normal_df_raw = pd.read_csv(normal_data_path, skiprows=1, low_memory=False)\n",
    "attack_df_raw = pd.read_csv(attack_data_path, low_memory=False)\n",
    "\n",
    "normal_df = clean_swat_data(normal_df_raw)\n",
    "attack_df = clean_swat_data(attack_df_raw)\n",
    "\n",
    "combined_df = pd.concat([normal_df, attack_df])\n",
    "combined_df.sort_index(inplace=True)\n",
    "\n",
    "# --- Feature Selection ---\n",
    "selected_features = [\n",
    "    'FIT101', 'LIT101', 'AIT201', 'AIT202', 'FIT201', 'AIT402',\n",
    "    'FIT401', 'LIT301', 'LIT401', 'AIT502', 'FIT501', 'PIT501'\n",
    "]\n",
    "subset_df = combined_df[selected_features + ['Label']].copy()\n",
    "print(f\"Data loaded and processed. Using {len(selected_features)} features.\")"
   ],
   "id": "d831480175b3ea00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and processed. Using 12 features.\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:04:17.244449Z",
     "start_time": "2025-07-21T15:04:17.150569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = subset_df.drop('Label', axis=1)\n",
    "labels = subset_df['Label']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "scaled_df = pd.DataFrame(features_scaled, columns=features.columns, index=features.index)\n",
    "scaled_df['Label'] = labels\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = 50\n",
    "\n",
    "X, y = create_windows_vectorized(scaled_df, window_size=WINDOW_SIZE, stride=STEP_SIZE)\n",
    "print(f\"Windowing complete. Shape of X: {X.shape}\")"
   ],
   "id": "93fa32fec147b439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorized windows created: 18934 sequences of shape (50, 12)\n",
      "Windowing complete. Shape of X: (18934, 50, 12)\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:04:18.763604Z",
     "start_time": "2025-07-21T15:04:17.291453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import collections\n",
    "\n",
    "# Reshape the 3D windowed data to 2D for SMOTE\n",
    "num_samples, window_size, num_features = X.shape\n",
    "X_reshaped = X.reshape(num_samples, window_size * num_features)\n",
    "\n",
    "print(\"Original dataset shape:\", collections.Counter(y))\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_reshaped, y)\n",
    "\n",
    "# Reshape the data back to 3D for the model\n",
    "X = X_resampled.reshape(-1, window_size, num_features)\n",
    "y = y_resampled\n",
    "\n",
    "print(\"Resampled dataset shape:\", collections.Counter(y))"
   ],
   "id": "86bc776cddf2d6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: Counter({np.int64(0): 17810, np.int64(1): 1124})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshreyas/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: Counter({np.int64(0): 17810, np.int64(1): 17810})\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:04:19.120033Z",
     "start_time": "2025-07-21T15:04:18.874532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_normal = X[y == 0]\n",
    "y_normal = y[y == 0]\n",
    "X_attack = X[y == 1]\n",
    "y_attack = y[y == 1]\n",
    "\n",
    "if len(X_attack) >= 10: # Ensure there are enough attack samples for a split\n",
    "    # Split Normal Data\n",
    "    X_normal_train, X_normal_temp, y_normal_train, y_normal_temp = train_test_split(X_normal, y_normal, test_size=0.2, random_state=42)\n",
    "    X_normal_val, X_normal_test, y_normal_val, y_normal_test = train_test_split(X_normal_temp, y_normal_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Split Attack Data\n",
    "    X_attack_train, X_attack_temp, y_attack_train, y_attack_temp = train_test_split(X_attack, y_attack, test_size=0.2, random_state=42)\n",
    "    X_attack_val, X_attack_test, y_attack_val, y_attack_test = train_test_split(X_attack_temp, y_attack_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Combine\n",
    "    X_train = np.concatenate([X_normal_train, X_attack_train])\n",
    "    y_train = np.concatenate([y_normal_train, y_attack_train])\n",
    "    X_val = np.concatenate([X_normal_val, X_attack_val])\n",
    "    y_val = np.concatenate([y_normal_val, y_attack_val])\n",
    "    X_test = np.concatenate([X_normal_test, X_attack_test])\n",
    "    y_test = np.concatenate([y_normal_test, y_attack_test])\n",
    "\n",
    "    # Shuffle\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "    print(\"New data split complete:\")\n",
    "    print(\"Training set label distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "    print(\"Validation set label distribution:\", dict(zip(*np.unique(y_val, return_counts=True))))\n",
    "    print(\"Test set label distribution:\", dict(zip(*np.unique(y_test, return_counts=True))))\n",
    "else:\n",
    "    print(f\"Error: Not enough attack samples ({len(X_attack)}) to perform a split.\")"
   ],
   "id": "f274499958435d7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New data split complete:\n",
      "Training set label distribution: {np.int64(0): np.int64(14248), np.int64(1): np.int64(14248)}\n",
      "Validation set label distribution: {np.int64(0): np.int64(1781), np.int64(1): np.int64(1781)}\n",
      "Test set label distribution: {np.int64(0): np.int64(1781), np.int64(1): np.int64(1781)}\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T15:04:19.296700Z",
     "start_time": "2025-07-21T15:04:19.201804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_path = '../data/processed'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(save_path, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(save_path, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(save_path, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(save_path, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(save_path, 'y_test.npy'), y_test)\n",
    "\n",
    "# This tells you the correct number to use in your train.py and evaluate.py scripts\n",
    "print(f\"Data saved. The INPUT_DIM for your model is: {X_train.shape[2]}\")"
   ],
   "id": "1eec7913a61d4b1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved. The INPUT_DIM for your model is: 12\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5db8d0a9c6c8a07d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
