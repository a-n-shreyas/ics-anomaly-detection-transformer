{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T17:24:18.280374Z",
     "start_time": "2025-07-10T17:24:18.265969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Sklearn imports for preprocessing and splitting\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# --- Path and Import Setup for your custom functions ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import your custom functions from preprocess.py\n",
    "from preprocess import clean_wadi_data, create_windows_vectorized\n",
    "\n",
    "print(\"✅ All libraries and custom functions imported.\")"
   ],
   "id": "4f591f091dfddeba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries and custom functions imported.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T17:24:33.949544Z",
     "start_time": "2025-07-10T17:24:18.795450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load and Clean Raw Data ---\n",
    "def find_header_row(file_path):\n",
    "    \"\"\"Opens a file and finds the line number of the real header.\"\"\"\n",
    "    with open(file_path, 'r', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line.startswith('Row,Date,Time'):\n",
    "                return i\n",
    "    return 0\n",
    "\n",
    "normal_path = os.path.join('..', 'data', 'WADI', 'WADI_14days.csv')\n",
    "attack_path = os.path.join('..', 'data', 'WADI', 'WADI_attackdata.csv')\n",
    "\n",
    "lines_to_skip_normal = find_header_row(normal_path)\n",
    "lines_to_skip_attack = find_header_row(attack_path)\n",
    "\n",
    "normal_df_raw = pd.read_csv(normal_path, skiprows=lines_to_skip_normal)\n",
    "attack_df_raw = pd.read_csv(attack_path, skiprows=lines_to_skip_attack)\n",
    "\n",
    "normal_df = clean_wadi_data(normal_df_raw.copy())\n",
    "attack_df = clean_wadi_data(attack_df_raw.copy())\n",
    "\n",
    "print(\"✅ Raw data loaded and cleaned successfully.\")\n",
    "print(f\"Normal df shape: {normal_df.shape}\")\n",
    "print(f\"Attack df shape: {attack_df.shape}\")"
   ],
   "id": "f4ffb2ae7630c3d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Raw data loaded and cleaned successfully.\n",
      "Normal df shape: (1209601, 127)\n",
      "Attack df shape: (172801, 127)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T17:24:36.357774Z",
     "start_time": "2025-07-10T17:24:33.988319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Combine Data, Apply Attack Labels, and Select Features ---\n",
    "combined_df = pd.concat([normal_df, attack_df])\n",
    "combined_df.sort_index(inplace=True)\n",
    "combined_df = combined_df.copy() # Create a clean, un-fragmented copy\n",
    "combined_df['Label'] = 0\n",
    "\n",
    "attack_intervals = [\n",
    "    ('2017-10-09 19:25:00', '2017-10-09 19:50:00'),\n",
    "    ('2017-10-10 10:25:00', '2017-10-10 10:35:00'),\n",
    "    ('2017-10-10 10:50:00', '2017-10-10 11:00:00'),\n",
    "    ('2017-10-10 11:20:00', '2017-10-10 11:30:00'),\n",
    "    ('2017-10-10 11:40:00', '2017-10-10 11:50:00'),\n",
    "    ('2017-10-10 14:30:00', '2017-10-10 14:40:00'),\n",
    "    ('2017-10-10 14:50:00', '2017-10-10 15:00:00'),\n",
    "    ('2017-10-10 15:20:00', '2017-10-10 15:30:00'),\n",
    "    ('2017-10-11 10:25:00', '2017-10-11 10:35:00'),\n",
    "    ('2017-10-11 10:55:00', '2017-10-11 11:05:00'),\n",
    "    ('2017-10-11 11:20:00', '2017-10-11 11:25:00'),\n",
    "    ('2017-10-11 11:40:00', '2017-10-11 11:45:00'),\n",
    "    ('2017-10-11 15:35:00', '2017-10-11 15:45:00'),\n",
    "    ('2017-10-11 15:55:00', '2017-10-11 16:00:00')\n",
    "]\n",
    "\n",
    "for start, end in attack_intervals:\n",
    "    start_ts = pd.to_datetime(start)\n",
    "    end_ts = pd.to_datetime(end)\n",
    "    mask = (combined_df.index >= start_ts) & (combined_df.index <= end_ts)\n",
    "    combined_df.loc[mask, 'Label'] = 1\n",
    "\n",
    "# --- Feature Selection ---\n",
    "selected_features = [\n",
    "    '1_AIT_001_PV', '1_AIT_002_PV', '1_AIT_003_PV', '1_AIT_004_PV', '1_AIT_005_PV',\n",
    "    '1_FIT_001_PV', '1_LT_001_PV', '2_DPIT_001_PV', '2_FIC_101_PV', '2_FIC_201_PV',\n",
    "    '2_FIC_301_PV', '2_FIC_401_PV', '2_FIC_501_PV', '2_FIC_601_PV',\n",
    "    '2_LT_001_PV', '2_LT_002_PV', '2_PIT_001_PV', '2_PIT_002_PV', '2_PIT_003_PV'\n",
    "]\n",
    "subset_df = combined_df[selected_features + ['Label']].copy()\n",
    "\n",
    "print(\"✅ Data combined, labeled, and features selected.\")\n",
    "print(f\"Final shape of subset_df: {subset_df.shape}\")"
   ],
   "id": "39f45601c598f082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data combined, labeled, and features selected.\n",
      "Final shape of subset_df: (1382402, 20)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T17:24:36.490503Z",
     "start_time": "2025-07-10T17:24:36.368942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Scale Features and Create Windows ---\n",
    "features = subset_df.drop('Label', axis=1)\n",
    "labels = subset_df['Label']\n",
    "\n",
    "# Drop any all-NaN columns that might exist\n",
    "features.dropna(axis=1, how='all', inplace=True)\n",
    "print(f\"Features to be scaled: {features.shape[1]}\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "print(\"✅ Features scaled successfully.\")\n",
    "\n",
    "# Create a temporary DataFrame to pass to the windowing function\n",
    "scaled_df = pd.DataFrame(features_scaled, columns=features.columns, index=features.index)\n",
    "scaled_df['Label'] = labels\n",
    "\n",
    "# Define windowing parameters\n",
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = 50 # Using non-overlapping windows for efficiency\n",
    "\n",
    "# Generate the windowed dataset using the efficient vectorized function\n",
    "X, y = create_windows_vectorized(scaled_df, window_size=WINDOW_SIZE, stride=STEP_SIZE)\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ],
   "id": "6c8a7f5504c910ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to be scaled: 19\n",
      "✅ Features scaled successfully.\n",
      "✅ Vectorized windows created: 27648 sequences of shape (50, 19)\n",
      "Shape of X: (27648, 50, 19)\n",
      "Shape of y: (27648,)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T17:24:36.696215Z",
     "start_time": "2025-07-10T17:24:36.539462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Manual Stratified Split ---\n",
    "X_normal = X[y == 0]\n",
    "y_normal = y[y == 0]\n",
    "X_attack = X[y == 1]\n",
    "y_attack = y[y == 1]\n",
    "\n",
    "# Split Normal Data (80/10/10)\n",
    "X_normal_train, X_normal_temp, y_normal_train, y_normal_temp = train_test_split(\n",
    "    X_normal, y_normal, test_size=0.2, random_state=42\n",
    ")\n",
    "X_normal_val, X_normal_test, y_normal_val, y_normal_test = train_test_split(\n",
    "    X_normal_temp, y_normal_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Split Attack Data (80/10/10)\n",
    "X_attack_train, X_attack_temp, y_attack_train, y_attack_temp = train_test_split(\n",
    "    X_attack, y_attack, test_size=0.2, random_state=42\n",
    ")\n",
    "X_attack_val, X_attack_test, y_attack_val, y_attack_test = train_test_split(\n",
    "    X_attack_temp, y_attack_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Combine the splits\n",
    "X_train = np.concatenate([X_normal_train, X_attack_train])\n",
    "y_train = np.concatenate([y_normal_train, y_attack_train])\n",
    "X_val = np.concatenate([X_normal_val, X_attack_val])\n",
    "y_val = np.concatenate([y_normal_val, y_attack_val])\n",
    "X_test = np.concatenate([X_normal_test, X_attack_test])\n",
    "y_test = np.concatenate([y_normal_test, y_attack_test])\n",
    "\n",
    "# Shuffle the training data\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# Verify the New Distribution\n",
    "print(\"✅ New data split complete:\")\n",
    "print(\"Training set label distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print(\"Validation set label distribution:\", dict(zip(*np.unique(y_val, return_counts=True))))\n",
    "print(\"Test set label distribution:\", dict(zip(*np.unique(y_test, return_counts=True))))"
   ],
   "id": "ef38a2d1931635e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New data split complete:\n",
      "Training set label distribution: {np.int64(0): np.int64(21972), np.int64(1): np.int64(145)}\n",
      "Validation set label distribution: {np.int64(0): np.int64(2747), np.int64(1): np.int64(18)}\n",
      "Test set label distribution: {np.int64(0): np.int64(2747), np.int64(1): np.int64(19)}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T17:24:37.499626Z",
     "start_time": "2025-07-10T17:24:36.699595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Save Processed Data ---\n",
    "save_path = '../data/processed'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(save_path, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(save_path, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(save_path, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(save_path, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(save_path, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"✅ All data arrays saved successfully to: {os.path.abspath(save_path)}\")"
   ],
   "id": "be1fcd74fe50e1bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All data arrays saved successfully to: /Users/anshreyas/PycharmProjects/ics-anomaly-detection-transformer/data/processed\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac8a36927f8a5630",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
