{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T18:44:14.658243Z",
     "start_time": "2025-07-08T18:44:10.478075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Combine Data and Fix PerformanceWarning ---\n",
    "combined_df = pd.concat([normal_df, attack_df])\n",
    "combined_df.sort_index(inplace=True)\n",
    "combined_df = combined_df.copy() # FIX: Create a clean copy\n",
    "combined_df['Label'] = 0\n",
    "\n",
    "\n",
    "# --- 1. Scale the Features ---\n",
    "features = subset_df.drop('Label', axis=1)\n",
    "\n",
    "features.dropna(axis=1, how='all', inplace=True)\n",
    "print(f\"✅ Dropped all-NaN columns. Remaining features: {features.shape[1]}\")\n",
    "\n",
    "labels = subset_df['Label']\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "print(\"✅ Features scaled successfully.\")\n",
    "\n",
    "# --- 2. Create Sliding Windows ---\n",
    "def create_windows(data, labels, window_size, step_size):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        end_idx = i + window_size\n",
    "        window_features = data[i:end_idx]\n",
    "        window_labels = labels[i:end_idx]\n",
    "        window_label = 1 if np.any(window_labels == 1) else 0\n",
    "        X.append(window_features)\n",
    "        y.append(window_label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define windowing parameters with a larger step size\n",
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = 50\n",
    "\n",
    "# Generate the windowed dataset\n",
    "X, y = create_windows(features_scaled, labels.values, WINDOW_SIZE, STEP_SIZE)\n",
    "print(f\"✅ Windowing complete. Created {len(X)} windows of size {WINDOW_SIZE}.\")\n",
    "\n",
    "# --- 3. Split into Train, Validation, and Test Sets ---\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(\"\\n✅ Data split complete:\")\n",
    "print(f\"Training set:   {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set:       {len(X_test)} samples\")"
   ],
   "id": "4f591f091dfddeba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dropped all-NaN columns. Remaining features: 19\n",
      "✅ Features scaled successfully.\n",
      "✅ Windowing complete. Created 27648 windows of size 50.\n",
      "\n",
      "✅ Data split complete:\n",
      "Training set:   22118 samples\n",
      "Validation set: 2765 samples\n",
      "Test set:       2765 samples\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T18:44:14.769652Z",
     "start_time": "2025-07-08T18:44:14.666006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selected_features = [\n",
    "    '1_AIT_001_PV', '1_AIT_002_PV', '1_AIT_003_PV', '1_AIT_004_PV', '1_AIT_005_PV',\n",
    "    '1_FIT_001_PV', '1_LT_001_PV', '2_DPIT_001_PV', '2_FIC_101_PV', '2_FIC_201_PV',\n",
    "    '2_FIC_301_PV', '2_FIC_401_PV', '2_FIC_501_PV', '2_FIC_601_PV',\n",
    "    '2_LT_001_PV', '2_LT_002_PV', '2_PIT_001_PV', '2_PIT_002_PV', '2_PIT_003_PV'\n",
    "]\n",
    "subset_df = combined_df[selected_features + ['Label']].copy()\n",
    "\n",
    "print(f\"Created a new dataset with {len(selected_features)} selected features.\")"
   ],
   "id": "f4ffb2ae7630c3d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new dataset with 19 selected features.\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T18:44:15.526049Z",
     "start_time": "2025-07-08T18:44:14.858555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "combined_df = pd.concat([normal_df, attack_df])\n",
    "combined_df.sort_index(inplace=True)\n",
    "\n",
    "combined_df['Label'] = 0\n",
    "\n",
    "attack_intervals = [\n",
    "    ('2017-10-09 19:25:00', '2017-10-09 19:50:00'),\n",
    "    ('2017-10-10 10:25:00', '2017-10-10 10:35:00'),\n",
    "    ('2017-10-10 10:50:00', '2017-10-10 11:00:00'),\n",
    "    ('2017-10-10 11:20:00', '2017-10-10 11:30:00'),\n",
    "    ('2017-10-10 11:40:00', '2017-10-10 11:50:00'),\n",
    "    ('2017-10-10 14:30:00', '2017-10-10 14:40:00'),\n",
    "    ('2017-10-10 14:50:00', '2017-10-10 15:00:00'),\n",
    "    ('2017-10-10 15:20:00', '2017-10-10 15:30:00'),\n",
    "    ('2017-10-11 10:25:00', '2017-10-11 10:35:00'),\n",
    "    ('2017-10-11 10:55:00', '2017-10-11 11:05:00'),\n",
    "    ('2017-10-11 11:20:00', '2017-10-11 11:25:00'),\n",
    "    ('2017-10-11 11:40:00', '2017-10-11 11:45:00'),\n",
    "    ('2017-10-11 15:35:00', '2017-10-11 15:45:00'),\n",
    "    ('2017-10-11 15:55:00', '2017-10-11 16:00:00')\n",
    "]\n",
    "\n",
    "for start, end in attack_intervals:\n",
    "    start_ts = pd.to_datetime(start)\n",
    "    end_ts = pd.to_datetime(end)\n",
    "    mask = (combined_df.index >= start_ts) & (combined_df.index <= end_ts)\n",
    "    combined_df.loc[mask, 'Label'] = 1\n",
    "\n",
    "\n",
    "# --- 5. Sanity Check ---\n",
    "print(\"✅ Labeling complete.\")\n",
    "print(\"Label distribution:\")\n",
    "print(combined_df['Label'].value_counts())\n",
    "\n",
    "print(\"\\nData with labels:\")\n",
    "print(combined_df.head())"
   ],
   "id": "39f45601c598f082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Labeling complete.\n",
      "Label distribution:\n",
      "Label\n",
      "0    1373988\n",
      "1       8414\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data with labels:\n",
      "                     1_AIT_001_PV  1_AIT_002_PV  1_AIT_003_PV  1_AIT_004_PV  \\\n",
      "Timestamp                                                                     \n",
      "2017-09-25 18:00:00       171.155      0.619473       11.5759       504.645   \n",
      "2017-09-25 18:00:01       171.155      0.619473       11.5759       504.645   \n",
      "2017-09-25 18:00:02       171.155      0.619473       11.5759       504.645   \n",
      "2017-09-25 18:00:03       171.155      0.607477       11.5725       504.673   \n",
      "2017-09-25 18:00:04       171.155      0.607477       11.5725       504.673   \n",
      "\n",
      "                     1_AIT_005_PV  1_FIT_001_PV  1_LS_001_AL  1_LS_002_AL  \\\n",
      "Timestamp                                                                   \n",
      "2017-09-25 18:00:00      0.318319      0.001157            0            0   \n",
      "2017-09-25 18:00:01      0.318319      0.001157            0            0   \n",
      "2017-09-25 18:00:02      0.318319      0.001157            0            0   \n",
      "2017-09-25 18:00:03      0.318438      0.001207            0            0   \n",
      "2017-09-25 18:00:04      0.318438      0.001207            0            0   \n",
      "\n",
      "                     1_LT_001_PV  1_MV_001_STATUS  ...  3_MV_002_STATUS  \\\n",
      "Timestamp                                          ...                    \n",
      "2017-09-25 18:00:00      47.8911                1  ...                1   \n",
      "2017-09-25 18:00:01      47.8911                1  ...                1   \n",
      "2017-09-25 18:00:02      47.8911                1  ...                1   \n",
      "2017-09-25 18:00:03      47.7503                1  ...                1   \n",
      "2017-09-25 18:00:04      47.7503                1  ...                1   \n",
      "\n",
      "                     3_MV_003_STATUS  3_P_001_STATUS  3_P_002_STATUS  \\\n",
      "Timestamp                                                              \n",
      "2017-09-25 18:00:00                1               1               1   \n",
      "2017-09-25 18:00:01                1               1               1   \n",
      "2017-09-25 18:00:02                1               1               1   \n",
      "2017-09-25 18:00:03                1               1               1   \n",
      "2017-09-25 18:00:04                1               1               1   \n",
      "\n",
      "                     3_P_003_STATUS  3_P_004_STATUS  LEAK_DIFF_PRESSURE  \\\n",
      "Timestamp                                                                 \n",
      "2017-09-25 18:00:00               1               1             67.9651   \n",
      "2017-09-25 18:00:01               1               1             67.9651   \n",
      "2017-09-25 18:00:02               1               1             67.9651   \n",
      "2017-09-25 18:00:03               1               1             67.1948   \n",
      "2017-09-25 18:00:04               1               1             67.1948   \n",
      "\n",
      "                     PLANT_START_STOP_LOG  TOTAL_CONS_REQUIRED_FLOW  Label  \n",
      "Timestamp                                                                   \n",
      "2017-09-25 18:00:00                     1                      0.68      0  \n",
      "2017-09-25 18:00:01                     1                      0.68      0  \n",
      "2017-09-25 18:00:02                     1                      0.68      0  \n",
      "2017-09-25 18:00:03                     1                      0.68      0  \n",
      "2017-09-25 18:00:04                     1                      0.68      0  \n",
      "\n",
      "[5 rows x 128 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s3/66l5gwv57612vfhcrcpg1y9h0000gn/T/ipykernel_15672/1174509506.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['Label'] = 0\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T18:44:15.905940Z",
     "start_time": "2025-07-08T18:44:15.545488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Scale the Features ---\n",
    "features = subset_df.drop('Label', axis=1)\n",
    "labels = combined_df['Label']\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "print(\"✅ Features scaled successfully.\")\n",
    "\n",
    "# --- 2. Create Sliding Windows ---\n",
    "def create_windows(data, labels, window_size, step_size):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - window_size, step_size):\n",
    "        end_idx = i + window_size\n",
    "        window_features = data[i:end_idx]\n",
    "        window_labels = labels[i:end_idx]\n",
    "        window_label = 1 if np.any(window_labels == 1) else 0\n",
    "        X.append(window_features)\n",
    "        y.append(window_label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "WINDOW_SIZE = 50\n",
    "STEP_SIZE = 50\n",
    "\n",
    "X, y = create_windows(features_scaled, labels.values, WINDOW_SIZE, STEP_SIZE)\n",
    "print(f\"✅ Windowing complete. Created {len(X)} windows of size {WINDOW_SIZE}.\")\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# --- 3. Split into Train, Validation, and Test Sets ---\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(\"\\n✅ Data split complete:\")\n",
    "print(f\"Training set:   {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set:       {len(X_test)} samples\")"
   ],
   "id": "6c8a7f5504c910ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features scaled successfully.\n",
      "✅ Windowing complete. Created 27648 windows of size 50.\n",
      "Shape of X: (27648, 50, 19)\n",
      "Shape of y: (27648,)\n",
      "\n",
      "✅ Data split complete:\n",
      "Training set:   22118 samples\n",
      "Validation set: 2765 samples\n",
      "Test set:       2765 samples\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T18:44:16.223190Z",
     "start_time": "2025-07-08T18:44:15.946204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Separate Normal and Attack Windows ---\n",
    "X_normal = X[y == 0]\n",
    "y_normal = y[y == 0]\n",
    "X_attack = X[y == 1]\n",
    "y_attack = y[y == 1]\n",
    "\n",
    "# --- 2. Split Normal Data (80/10/10 split) ---\n",
    "X_normal_train, X_normal_temp, y_normal_train, y_normal_temp = train_test_split(\n",
    "    X_normal, y_normal, test_size=0.2, random_state=42\n",
    ")\n",
    "X_normal_val, X_normal_test, y_normal_val, y_normal_test = train_test_split(\n",
    "    X_normal_temp, y_normal_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# --- 3. Split Attack Data (80/10/10 split) ---\n",
    "X_attack_train, X_attack_temp, y_attack_train, y_attack_temp = train_test_split(\n",
    "    X_attack, y_attack, test_size=0.2, random_state=42\n",
    ")\n",
    "X_attack_val, X_attack_test, y_attack_val, y_attack_test = train_test_split(\n",
    "    X_attack_temp, y_attack_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# --- 4. Combine the splits ---\n",
    "X_train = np.concatenate([X_normal_train, X_attack_train])\n",
    "y_train = np.concatenate([y_normal_train, y_attack_train])\n",
    "\n",
    "X_val = np.concatenate([X_normal_val, X_attack_val])\n",
    "y_val = np.concatenate([y_normal_val, y_attack_val])\n",
    "\n",
    "X_test = np.concatenate([X_normal_test, X_attack_test])\n",
    "y_test = np.concatenate([y_normal_test, y_attack_test])\n",
    "\n",
    "# --- 5. Shuffle the training data ---\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "\n",
    "# --- 6. Verify the New Distribution ---\n",
    "print(\"✅ New data split complete:\")\n",
    "print(\"Training set label distribution:\", dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print(\"Validation set label distribution:\", dict(zip(*np.unique(y_val, return_counts=True))))\n",
    "print(\"Test set label distribution:\", dict(zip(*np.unique(y_test, return_counts=True))))"
   ],
   "id": "ef38a2d1931635e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New data split complete:\n",
      "Training set label distribution: {np.int64(0): np.int64(21972), np.int64(1): np.int64(145)}\n",
      "Validation set label distribution: {np.int64(0): np.int64(2747), np.int64(1): np.int64(18)}\n",
      "Test set label distribution: {np.int64(0): np.int64(2747), np.int64(1): np.int64(19)}\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T18:44:16.323809Z",
     "start_time": "2025-07-08T18:44:16.260974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "save_path = '../data/processed'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(save_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(save_path, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(save_path, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(save_path, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(save_path, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(save_path, 'y_test.npy'), y_test)\n",
    "\n",
    "print(f\"✅ All data arrays saved successfully to: {os.path.abspath(save_path)}\")"
   ],
   "id": "be1fcd74fe50e1bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All data arrays saved successfully to: /Users/anshreyas/PycharmProjects/ics-anomaly-detection-transformer/data/processed\n"
     ]
    }
   ],
   "execution_count": 65
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
